[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "50und.github.io",
    "section": "",
    "text": "Using Shap Values to Explain Tree Models\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nStephen Coley\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Forecasting_Crypto/index.html",
    "href": "posts/Forecasting_Crypto/index.html",
    "title": "Using Shap Values to Explain Tree Models",
    "section": "",
    "text": "Machine learning models have a reputation for being “black boxes.” This is a problem because it’s very hard to trust a model if you don’t know why it makes the decisions it makes. For certain applications this can actually make models unusable because the risk of the model doing something unexpected is too high. This is where shap values come in. Shap values help open up the black box by calculating how much each feature contribute’s to the model’s prediction.\nTo demonstrate, I trained a random forest regression model on the housing dataset. For the sake of making this a simple demonstration, I only kept numeric features in the dataset, and trained the model on all the data instead of doing a train test validation approach.\n\n\nCode\nimport pandas as pd\nimport shap\nimport numpy as np\n\ndat = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\ndat.head()\ndat['date'] = pd.to_datetime(dat['date'])\ndat = dat.drop(columns = ['id', 'zipcode', 'lat', 'long', 'date'], axis = 1)\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# prep data\ny = dat['price']\nX = dat.drop(columns = 'price', axis = 1)\n\nforest = RandomForestRegressor(n_estimators=100, max_depth=10)\n\n# scores=[]\n# kFold=KFold(n_splits=10,shuffle=False)\n\n# cv = cross_val_score(forest, X, y, cv=10)\n# print(np.mean(cv))\n\nforest.fit(X,y)"
  },
  {
    "objectID": "posts/Forecasting_Crypto/index.html#opening-the-black-box",
    "href": "posts/Forecasting_Crypto/index.html#opening-the-black-box",
    "title": "Using Shap Values to Explain Tree Models",
    "section": "",
    "text": "Machine learning models have a reputation for being “black boxes.” This is a problem because it’s very hard to trust a model if you don’t know why it makes the decisions it makes. For certain applications this can actually make models unusable because the risk of the model doing something unexpected is too high. This is where shap values come in. Shap values help open up the black box by calculating how much each feature contribute’s to the model’s prediction.\nTo demonstrate, I trained a random forest regression model on the housing dataset. For the sake of making this a simple demonstration, I only kept numeric features in the dataset, and trained the model on all the data instead of doing a train test validation approach.\n\n\nCode\nimport pandas as pd\nimport shap\nimport numpy as np\n\ndat = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\ndat.head()\ndat['date'] = pd.to_datetime(dat['date'])\ndat = dat.drop(columns = ['id', 'zipcode', 'lat', 'long', 'date'], axis = 1)\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# prep data\ny = dat['price']\nX = dat.drop(columns = 'price', axis = 1)\n\nforest = RandomForestRegressor(n_estimators=100, max_depth=10)\n\n# scores=[]\n# kFold=KFold(n_splits=10,shuffle=False)\n\n# cv = cross_val_score(forest, X, y, cv=10)\n# print(np.mean(cv))\n\nforest.fit(X,y)"
  },
  {
    "objectID": "posts/Forecasting_Crypto/index.html#summary-plots",
    "href": "posts/Forecasting_Crypto/index.html#summary-plots",
    "title": "Using Shap Values to Explain Tree Models",
    "section": "Summary Plots",
    "text": "Summary Plots\n\n\nCode\nexplainer = shap.TreeExplainer(forest)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n\n\n\n\n\nShap Summary Plot\n\n\nTo see an overview of what features are impacting the predictions the most, the shap library has an easy to use summary_plot function. We can see that the model gets the largest contributions in order from the grade of the home construction, square footage of the living space, the year the home was built, and the living space of the nearest 15 neighboring homes."
  },
  {
    "objectID": "posts/Forecasting_Crypto/index.html#dependence-plots",
    "href": "posts/Forecasting_Crypto/index.html#dependence-plots",
    "title": "Using Shap Values to Explain Tree Models",
    "section": "Dependence Plots",
    "text": "Dependence Plots\n\n\nCode\nfor i in range(0, len(X.columns)):\n    shap.dependence_plot(i, shap_values, X)\n\n\nTo dig into how these 4 features are affecting the model’s predictions, I am using shap’s dependence plot function, which plots the contributions of the feature color coded by the additional feature that is the most likely to have an interaction.\n\n\n\nGrade\n\n\nThe grade of the house is defined as the quality of its construction. It ranges in value from 1 to 13. This graph shows a positive relation between the predicted value of the house and the grade of the house, where houses with higher quality of construction also have higher property values. It makes sense that homes that are built better would be worth more, so we can say the model is predicting these values as expected. When we look at the sqft_living interaction, we see that most of the smallest homes also have the lowest grades and thus the lowest shap contributions. There is an interesting interaction we see around the grade 6 7 and 8 homes where some of the smaller homes have a higher shap contribution than smaller homes with the same grade. This is not what we would expect to see, so there is probably another unseen feature at play there. The relationship changes at grade 9 and above, with larger homes having higher predicted values than smaller homes at the same grade.\n\n\n\nSqft Living\n\n\nAt first glance, this chart tells us that the model wants to predict higher prices for larger homes, which is exactly what we would expect it to do. Again we see the tinge of purple blue corresponding to lower home grade providing higher shap values than homes of the same size that are at a higher grade. This relationship seems to disappear on homes larger than 4000 sqft, but it is concerning to see it so pronounced on the smaller homes.\n\n\n\nYear Built\n\n\nThis chart shows a negative relationship between the year the home was built and the shap values for the year. The model is wanting to predict lower values for newer homes than for older homes. This pattern is not what I would think the model should be doing, and merits further investigation. Our last plot makes this relationship make a little more sense.\n\n\n\nSqft Living of Nearest 15 Homes\n\n\nThis plot shows the most obvious interaction of any of the others. There is a positive relationship between the size of the nearest 15 homes and the price the model predicts, but this becomes much more pronounced with the older homes, as we can see by the much more extreme slope of the blue/purple points corresponding to older homes vs the red points corresponding to newer homes. The model seems to want to predict much lower values for older homes with smaller neighboring homes than for older homes with large neighboring homes. This makes our last plot makes sense, it seems that the older homes have higher values than the newer homes as long as the neighboring homes are large, but not if the neighboring homes are small. This is likely because the old homes with large neighbors are not only currently in a wealthy area but seem to have been in a wealthy area for a long time, and location is extremely important in home valuation."
  },
  {
    "objectID": "posts/Forecasting_Crypto/index.html#conclusion",
    "href": "posts/Forecasting_Crypto/index.html#conclusion",
    "title": "Using Shap Values to Explain Tree Models",
    "section": "Conclusion",
    "text": "Conclusion\nShap values allow you to dig into your machine learning models and gain an understanding of how they are using your features. This leads to much more understandable and therefore more useful models. I encourage the use of shap values in your next machine learning project."
  }
]